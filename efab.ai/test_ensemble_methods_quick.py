#!/usr/bin/env python3
"""
Quick Test for Advanced Ensemble Methods
Beverly Knits AI Supply Chain Planner
"""

import numpy as np
import sys
import os
import logging
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.ensemble import (
    StackingEnsemble,
    BlendingEnsemble,
    VotingEnsemble,
    EnsembleOptimizer,
    evaluate_ensemble
)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_data():
    """Create simple test data"""
    X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)
    return train_test_split(X, y, test_size=0.3, random_state=42)

def test_all_ensemble_methods():
    """Test all ensemble methods quickly"""
    logger.info("üöÄ Testing All Ensemble Methods (Quick Test)")
    
    try:
        # Create test data
        X_train, X_test, y_train, y_test = create_test_data()
        
        # Create simple base models
        base_models = {
            'linear': LinearRegression(),
            'ridge': Ridge(alpha=1.0),
            'rf': RandomForestRegressor(n_estimators=20, random_state=42)
        }
        
        results = {}
        
        # Test Stacking Ensemble
        logger.info("Testing Stacking Ensemble...")
        stacking_ensemble = StackingEnsemble(
            base_models=base_models,
            cv_folds=3,
            random_state=42
        )
        stacking_ensemble.fit(X_train, y_train)
        stacking_pred = stacking_ensemble.predict(X_test)
        results['stacking'] = {
            'mse': mean_squared_error(y_test, stacking_pred),
            'r2': r2_score(y_test, stacking_pred)
        }
        logger.info(f"Stacking - MSE: {results['stacking']['mse']:.4f}, R¬≤: {results['stacking']['r2']:.4f}")
        
        # Test Blending Ensemble
        logger.info("Testing Blending Ensemble...")
        blending_ensemble = BlendingEnsemble(
            base_models=base_models,
            blend_method='weighted',
            random_state=42
        )
        blending_ensemble.fit(X_train, y_train)
        blending_pred = blending_ensemble.predict(X_test)
        results['blending'] = {
            'mse': mean_squared_error(y_test, blending_pred),
            'r2': r2_score(y_test, blending_pred)
        }
        logger.info(f"Blending - MSE: {results['blending']['mse']:.4f}, R¬≤: {results['blending']['r2']:.4f}")
        
        # Test Voting Ensemble
        logger.info("Testing Voting Ensemble...")
        voting_ensemble = VotingEnsemble(
            base_models=base_models,
            voting_method='soft'
        )
        voting_ensemble.fit(X_train, y_train)
        voting_pred = voting_ensemble.predict(X_test)
        results['voting'] = {
            'mse': mean_squared_error(y_test, voting_pred),
            'r2': r2_score(y_test, voting_pred)
        }
        logger.info(f"Voting - MSE: {results['voting']['mse']:.4f}, R¬≤: {results['voting']['r2']:.4f}")
        
        # Test individual base models for comparison
        logger.info("Testing Base Models for comparison...")
        for name, model in base_models.items():
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            results[name] = {
                'mse': mean_squared_error(y_test, pred),
                'r2': r2_score(y_test, pred)
            }
            logger.info(f"{name} - MSE: {results[name]['mse']:.4f}, R¬≤: {results[name]['r2']:.4f}")
            
        # Find best method
        best_method = min(results.keys(), key=lambda x: results[x]['mse'])
        logger.info(f"\nBest method: {best_method}")
        logger.info(f"Best MSE: {results[best_method]['mse']:.4f}")
        logger.info(f"Best R¬≤: {results[best_method]['r2']:.4f}")
        
        # Test feature importance for stacking
        importance = stacking_ensemble.get_feature_importance()
        logger.info(f"Stacking feature importance: {len(importance)} features")
        
        # Test blend weights for blending
        weights = blending_ensemble.get_blend_weights()
        logger.info(f"Blending weights: {weights}")
        
        # Check that ensemble methods generally perform better than individual models
        ensemble_methods = ['stacking', 'blending', 'voting']
        base_methods = ['linear', 'ridge', 'rf']
        
        best_ensemble_mse = min(results[method]['mse'] for method in ensemble_methods)
        best_base_mse = min(results[method]['mse'] for method in base_methods)
        
        improvement = (best_base_mse - best_ensemble_mse) / best_base_mse * 100
        logger.info(f"Best ensemble improvement over best base model: {improvement:.2f}%")
        
        logger.info("‚úÖ All ensemble methods test completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Ensemble methods test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ensemble_optimizer_simple():
    """Test ensemble optimizer with simple configuration"""
    logger.info("üöÄ Testing Ensemble Optimizer (Simple)")
    
    try:
        # Create test data
        X_train, X_test, y_train, y_test = create_test_data()
        
        # Create optimizer
        optimizer = EnsembleOptimizer(random_state=42)
        
        # Create simple base models
        base_models = {
            'linear': LinearRegression(),
            'ridge': Ridge(alpha=1.0),
            'rf': RandomForestRegressor(n_estimators=10, random_state=42)
        }
        
        # Test only stacking and blending (skip voting due to cloning issues)
        logger.info("Optimizing ensemble methods...")
        optimization_result = optimizer.optimize_ensemble(
            X_train, y_train, base_models,
            ensemble_types=['stacking', 'blending'],
            cv_folds=3
        )
        
        # Get best ensemble
        best_ensemble = optimization_result['best_ensemble']
        best_name = optimization_result['best_ensemble_name']
        best_score = optimization_result['best_score']
        
        logger.info(f"Best ensemble: {best_name} (CV MSE: {best_score:.4f})")
        
        # Train and evaluate best ensemble
        best_ensemble.fit(X_train, y_train)
        test_metrics = evaluate_ensemble(best_ensemble, X_test, y_test)
        
        logger.info("Test metrics:")
        for metric, value in test_metrics.items():
            logger.info(f"  {metric}: {value:.4f}")
            
        # Show all results
        logger.info("\nAll ensemble results:")
        for name, result in optimization_result['all_results'].items():
            logger.info(f"  {name}: CV MSE={result['cv_score']:.4f}")
            
        logger.info("‚úÖ Ensemble optimizer test completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Ensemble optimizer test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run quick ensemble tests"""
    logger.info("üöÄ Running Quick Ensemble Methods Tests")
    
    tests = [
        test_all_ensemble_methods,
        test_ensemble_optimizer_simple
    ]
    
    results = []
    
    for test in tests:
        try:
            result = test()
            results.append(result)
            logger.info(f"Test {test.__name__}: {'PASSED' if result else 'FAILED'}")
        except Exception as e:
            logger.error(f"Test {test.__name__} failed with exception: {e}")
            results.append(False)
            
    # Overall result
    passed = sum(results)
    total = len(results)
    
    logger.info(f"\n=== QUICK ENSEMBLE TESTS SUMMARY ===")
    logger.info(f"Passed: {passed}/{total}")
    logger.info(f"Success rate: {passed/total*100:.1f}%")
    
    if passed == total:
        logger.info("‚úÖ All quick ensemble tests passed!")
        return True
    else:
        logger.error("‚ùå Some ensemble tests failed!")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)